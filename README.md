# 算法工程师面试问题总结（最新版）
What's in a name? That which we call a rose/by any other name would smell as sweet.


## [专栏地址：百面算法工程师——总结最新各种计算机视觉的相关算法面试问题](https://blog.csdn.net/m0_67647321/category_12647126.html)



<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/137941855">百面算法工程师 | 损失函数篇</a></div>


<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502">百面算法工程师 | python解释器相关</div>


<div align="center">百面算法工程师 | 零碎知识点 【待更新】</div>


<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/137997215">百面算法工程师 | 经典分类网络总结</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138272899">百面算法工程师 | 目标检测总结</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138485652">百面算法工程师 | YOLOv5面试考点原理全解析</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138846084">百面算法工程师 | YOLOv6面试考点原理全解析</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138956713">百面算法工程师 | YOLOv8面试考点原理全解析</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138525231">百面算法工程师 | 深度学习目标检测岗位面试总结</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138072635">百面算法工程师 | 分类和聚类</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138216069">百面算法工程师 | Transformer</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138071676">百面算法工程师 | 卷积基础知识Conv</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138121965">百面算法工程师 | 分割网络总结</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138156023">百面算法工程师 | 激活函数</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138156239">百面算法工程师 | 优化函数</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138187377">百面算法工程师 | 深度学习基础理论</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138681102?spm=1001.2014.3001.5501">百面算法工程师 | 传统图像算法</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138309213">百面算法工程师 | 池化层</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138397057">百面算法工程师 | 支持向量机</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138610574">百面算法工程师 | 模型评价指标</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138707720">百面算法工程师 | 正则优化函数——BN、LN、Dropout</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138794101">百面算法工程师 | 特征工程相关理论</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502">百面算法工程师 | 降维</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/139065255">百面算法工程师 | 模型优化</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/139090145">百面算法工程师 | 集成学习——Boosting&Bagging</div>

<div align="center"><a href="https://blog.csdn.net/m0_67647321/article/details/139112830">百面算法工程师 | 贝叶斯网络</div>



| [YOLOv5面试考点原理全解析](https://blog.csdn.net/m0_67647321/article/details/138485652) | [数据增强模块](https://blog.csdn.net/m0_67647321/article/details/138485652#t0)<br/>[网络结构](https://blog.csdn.net/m0_67647321/article/details/138485652#t1)<br />[正负样本匹配](https://blog.csdn.net/m0_67647321/article/details/138485652#t2)<br/>[LOSS](https://blog.csdn.net/m0_67647321/article/details/138485652#t3)<br/>[优化策略和训练过程](https://blog.csdn.net/m0_67647321/article/details/138485652#t4)<br/>[推理和后处理过程](https://blog.csdn.net/m0_67647321/article/details/138485652#t4) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [ YOLOv8面试考点原理全解析](https://blog.csdn.net/m0_67647321/article/details/138956713) | [1. 网络结构](https://blog.csdn.net/m0_67647321/article/details/138956713#t0)<br />[2. Loss 计算](https://blog.csdn.net/m0_67647321/article/details/138956713#t1)<br />[3. 训练数据增强](https://blog.csdn.net/m0_67647321/article/details/138956713#t2)<br />[4. 训练策略](https://blog.csdn.net/m0_67647321/article/details/138956713#t3)<br />[5. 模型推理过程](https://blog.csdn.net/m0_67647321/article/details/138956713#t4) |
| [YOLOv6面试考点原理全解析](https://blog.csdn.net/m0_67647321/article/details/138846084) | [1. 数据增强模块](https://blog.csdn.net/m0_67647321/article/details/138846084#t0)<br />[2. 网络结构](https://blog.csdn.net/m0_67647321/article/details/138846084#t1)<br />[3. 正负样本匹配策略](https://blog.csdn.net/m0_67647321/article/details/138846084#t2)<br />[4. Loss 设计](https://blog.csdn.net/m0_67647321/article/details/138846084#t3) |
| [损失相关](https://blog.csdn.net/m0_67647321/article/details/137941855) | 1.1 Focal Loss[主要针对one stage]<br />1.2 DFL（YOLOv8损失函数）<br />1.3 IoU<br />1.4 GIoU<br />1.5 DIoU<br />1.6 CIoU<br />1.7 SIoU |
| [python解释器相关](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502) | [2.1 Python的装饰器的作用是什么，为什么要这么做](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.1 Python的装饰器的作用是什么，为什么要这么做)<br />[2.2 什么是解释性语言，什么是编译性语言](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.2 什么是解释性语言，什么是编译性语言)<br />[2.3 python程序的执行过程](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.3 python程序的执行过程)<br />[2.4 python的作用域](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.4 python的作用域)<br />[2.5 python的数据结构](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.5 python的数据结构)<br />[2.6 python多线程](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.6 python多线程)<br />[2.7 python多进程](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.7 python多进程)<br />[2.8 Python互斥锁与死锁](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.8 Python互斥锁与死锁)<br />[2.9 Python的深拷贝与浅拷贝](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.9 Python的深拷贝与浅拷贝)<br />[2.10 hasattr() getattr() setattr() 函数使用详解](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.10 hasattr() getattr() setattr() 函数使用详解)<br />[2.11 \__init__.py 文件的作用以及意义](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.11 __init__.py 文件的作用以及意义)<br />[2.12 点积和矩阵相乘的区别](https://blog.csdn.net/m0_67647321/article/details/138567852?spm=1001.2014.3001.5502#2.12 点积和矩阵相乘的区别) |
| 零碎知识点                                                   | 3.1 BN<br />3.2 双线性插值<br />3.3 为什么传统CNN的输入图片是固定大小 |
| 经典分类网络与发展                                           | [4.1 AlexNet](https://blog.csdn.net/m0_67647321/article/details/137997215#4.1 AlexNet)<br />[4.2 VGGNet](https://blog.csdn.net/m0_67647321/article/details/137997215#4.2 VGGNet)<br />[4.3 GoogLeNet](https://blog.csdn.net/m0_67647321/article/details/137997215#4.3 GoogLeNet)<br />[4.4 ResNet](https://blog.csdn.net/m0_67647321/article/details/137997215#4.4 ResNet)<br />[4.5 DenseNet](https://blog.csdn.net/m0_67647321/article/details/137997215#4.5 DenseNet)<br />[4.6 MobileNet](https://blog.csdn.net/m0_67647321/article/details/137997215#4.6 MobileNet)<br />[4.7 ShuffleNet](https://blog.csdn.net/m0_67647321/article/details/137997215#4.7 ShuffleNet)<br />[4.8 SENet（ImageNet最后一届竞赛的冠军，提出了SE结构）](https://blog.csdn.net/m0_67647321/article/details/137997215#4.8 SENet（ImageNet最后一届竞赛的冠军，提出了SE结构）) |
| object detect                                                | [5.1 Single Shot MultiBox Detector（SSD）](https://blog.csdn.net/m0_67647321/article/details/138272899#5.1 Single Shot MultiBox Detector（SSD）)<br />[5.2 YOLO](https://blog.csdn.net/m0_67647321/article/details/138272899#5.2 YOLO)<br />[v1](https://blog.csdn.net/m0_67647321/article/details/138272899#5.2.1 v1)<br />[v2](https://blog.csdn.net/m0_67647321/article/details/138272899#5.2.2 v2)<br />[v3](https://blog.csdn.net/m0_67647321/article/details/138272899#5.2.3 v3)<br />[v4](https://blog.csdn.net/m0_67647321/article/details/138272899#5.2.4 v4)<br />[v5](https://blog.csdn.net/m0_67647321/article/details/138485652?spm=1001.2014.3001.5501)<br />[v6](https://blog.csdn.net/m0_67647321/article/details/138846084)<br />[v7](https://blog.csdn.net/m0_67647321/article/details/138272899#5.2.7 v7)<br />[v8](https://blog.csdn.net/m0_67647321/article/details/138956713)<br />[v9](https://blog.csdn.net/m0_67647321/article/details/138272899#5.2.9 v9)<br />[v10](https://arxiv.org/abs/2405.14458)<br />[5.3 NMS](https://blog.csdn.net/m0_67647321/article/details/138272899#5.3 NMS)<br />[5.4 Anchor](https://blog.csdn.net/m0_67647321/article/details/138272899#5.4 Anchor)<br />[5.5 类别不均衡](https://blog.csdn.net/m0_67647321/article/details/138272899#5.5 类别不均衡)<br />[5.6 anchor free FCOS ](https://blog.csdn.net/m0_67647321/article/details/138272899#5.6 anchor free FCOS)<br />[5.7 YOLOX](https://blog.csdn.net/m0_67647321/article/details/138272899#5.7 YOLOX) |
| 分类和聚类                                                   | [6.1 为什么正确率有时不能有效评估分类算法？](https://blog.csdn.net/m0_67647321/article/details/138072635#6.1 为什么正确率有时不能有效评估分类算法？)<br />[6.2 什么样的分类器最好？](https://blog.csdn.net/m0_67647321/article/details/138072635#6.2 什么样的分类器最好？)<br />[6.3 什么是聚类，你知道哪些聚类算法？](https://blog.csdn.net/m0_67647321/article/details/138072635#6.3 什么是聚类，你知道哪些聚类算法？)<br />[6.4 K-Means聚类算法如何调优?](https://blog.csdn.net/m0_67647321/article/details/138072635#6.4 K-Means聚类算法如何调优%3F)<br />[6.5 K-Means聚类算法如何选择初始点?](https://blog.csdn.net/m0_67647321/article/details/138072635#6.5 K-Means聚类算法如何选择初始点%3F)<br />[6.6 K-Means聚类聚的是特征还是样本](https://blog.csdn.net/m0_67647321/article/details/138072635#6.6 K-Means聚类聚的是特征还是样本)<br />[6.7 K-Means++](https://blog.csdn.net/m0_67647321/article/details/138072635#6.7 K-Means%2B%2B) |
| Transformer                                                  | [7.1 Encoder](https://blog.csdn.net/m0_67647321/article/details/138216069#7.1 Encoder)<br />[7.2 Decoder](https://blog.csdn.net/m0_67647321/article/details/138216069#7.2 Decoder)<br />[7.3 训练与测试阶段Decoder的输入、输出](https://blog.csdn.net/m0_67647321/article/details/138216069#7.3 训练与测试阶段Decoder的输入、输出)<br />[7.4 Transformer Encoder和Decoder的区别](https://blog.csdn.net/m0_67647321/article/details/138216069#7.4 Transformer Encoder和Decoder的区别)<br />[7.5 Transformer中的Embedding](https://blog.csdn.net/m0_67647321/article/details/138216069#7.5 Transformer中的Embedding)<br />[7.6 Positional Embedding](https://blog.csdn.net/m0_67647321/article/details/138216069#7.6 Positional Embedding)<br />[7.7 Transformer中的Attention以及Q、K、V](https://blog.csdn.net/m0_67647321/article/details/138216069#7.7 Transformer中的Attention以及Q、K、V)<br />[7.8 Transformer中的Multi head Attention](https://blog.csdn.net/m0_67647321/article/details/138216069#7.8 Transformer中的Multi head Attention)<br />[7.9 Mask Multi head Attention](https://blog.csdn.net/m0_67647321/article/details/138216069#7.9 Mask Multi head Attention)<br />[7.10 self-attention](https://blog.csdn.net/m0_67647321/article/details/138216069#7.10 self-attention) |
| 卷积Conv                                                     | [8.1 图像卷积过程](https://blog.csdn.net/m0_67647321/article/details/138071676#8.1 图像卷积过程)<br />[8.2 卷积层基本参数](https://blog.csdn.net/m0_67647321/article/details/138071676#8.2 卷积层基本参数)<br />[8.3 卷积后图像的长和宽大小的计算方式](https://blog.csdn.net/m0_67647321/article/details/138071676#8.3 卷积后图像的长和宽大小的计算方式)<br />[8.4 卷积神经网络中的权重共享](https://blog.csdn.net/m0_67647321/article/details/138071676#8.4 卷积神经网络中的权重共享)<br />[8.5 上采样中的反卷积](https://blog.csdn.net/m0_67647321/article/details/138071676#8.5 上采样中的反卷积)<br />[8.6 空洞卷积](https://blog.csdn.net/m0_67647321/article/details/138071676#8.6 空洞卷积)<br />[8.7 深度可分离卷积](https://blog.csdn.net/m0_67647321/article/details/138071676#8.7 深度可分离卷积)<br />[8.8 为什么可分离卷积中Depthwise卷积后还要进行pointwise卷积](https://blog.csdn.net/m0_67647321/article/details/138071676#8.8 为什么可分离卷积中Depthwise卷积后还要进行pointwise卷积)<br />[8.9 分组卷积 Group Conv](https://blog.csdn.net/m0_67647321/article/details/138071676#8.9 分组卷积 Group Conv)<br />[8.10 1x1卷积作用](https://blog.csdn.net/m0_67647321/article/details/138071676#8.10 1x1卷积作用)<br />[8.11 卷积的底层实现/加速技巧](https://blog.csdn.net/m0_67647321/article/details/138071676#8.11 卷积的底层实现%2F加速技巧)<br />[8.12 卷积神经网络的特点](https://blog.csdn.net/m0_67647321/article/details/138071676#8.12 卷积神经网络的特点)<br />[8.13 卷积的memory，params，GFLOPs计算方法](https://blog.csdn.net/m0_67647321/article/details/138071676#8.13 卷积的memory，params，GFLOPs计算方法) |
| 分割网络                                                     | [9.1 语义分割](https://blog.csdn.net/m0_67647321/article/details/138121965#9.1 语义分割)<br />[9.2 实例分割](https://blog.csdn.net/m0_67647321/article/details/138121965#9.2 实例分割)<br />[9.3 为什么传统CNN的输入图片是固定大小](https://blog.csdn.net/m0_67647321/article/details/138121965#9.3 为什么传统CNN的输入图片是固定大小)<br />[9.4 FCN](https://blog.csdn.net/m0_67647321/article/details/138121965#9.4 FCN)<br />[9.5 SegNet](https://blog.csdn.net/m0_67647321/article/details/138121965#9.5 SegNet)<br />[9.6 使用池化层进行上采样的优势](https://blog.csdn.net/m0_67647321/article/details/138121965#9.6 使用池化层进行上采样的优势)<br />[9.7 UNet](https://blog.csdn.net/m0_67647321/article/details/138121965#9.7 UNet)<br />[9.8 PSPNet](https://blog.csdn.net/m0_67647321/article/details/138121965#9.8 PSPNet)<br />[9.9 DeepLab v1 v2 v3](https://blog.csdn.net/m0_67647321/article/details/138121965#9.9 DeepLab v1 v2 v3)<br />[9.10 Mask R-CNN](https://blog.csdn.net/m0_67647321/article/details/138121965#9.10 Mask R-CNN)<br />[9.11 RoIAlign](https://blog.csdn.net/m0_67647321/article/details/138121965#9.10 RoIAlign) |
| 激活函数 Activate Function                                   | [10.1激活函数作用](https://blog.csdn.net/m0_67647321/article/details/138156023# 10.1激活函数作用：)<br />[10.2 为什么激活函数都是非线性的](https://blog.csdn.net/m0_67647321/article/details/138156023#10.2 为什么激活函数都是非线性的)<br />[10.3 常见激活函数的优缺点及其取值范围](https://blog.csdn.net/m0_67647321/article/details/138156023#10.3 常见激活函数的优缺点及其取值范围)<br />[10.4 激活函数问题的汇总](https://blog.csdn.net/m0_67647321/article/details/138156023#10.4 激活函数问题的汇总)<br />[10.5 如何选择激活函数](https://blog.csdn.net/m0_67647321/article/details/138156023#10.5 如何选择激活函数)<br />[10.6 激活函数有哪些性质](https://blog.csdn.net/m0_67647321/article/details/138156023#10.6 激活函数有哪些性质) |
| 优化函数                                                     | [11.1 优化函数的作用](https://blog.csdn.net/m0_67647321/article/details/138156239#11.1 优化函数的作用)<br />[11.2 梯度下降法的作用](https://blog.csdn.net/m0_67647321/article/details/138156239#11.2 梯度下降法的作用)<br />[11.3 优化函数及其优缺点](https://blog.csdn.net/m0_67647321/article/details/138156239#11.3 优化函数及其优缺点)<br />[11.4 SGD和Adam的对比](https://blog.csdn.net/m0_67647321/article/details/138156239#11.4 SGD和Adam的对比)<br />[11.5 Batch的影响](https://blog.csdn.net/m0_67647321/article/details/138156239#11.5 Batch的影响) |
| 基础理论                                                     | [12.1 超参数调优](https://blog.csdn.net/m0_67647321/article/details/138187377#12.1 超参数调优)<br />[12.2 为什么需要Batch Size](https://blog.csdn.net/m0_67647321/article/details/138187377#12.2 为什么需要Batch Size)<br />[12.3 归一化的目的](https://blog.csdn.net/m0_67647321/article/details/138187377#12.3 归一化的目的)<br />[12.4 局部最优与全局最优](https://blog.csdn.net/m0_67647321/article/details/138187377#12.4 局部最优与全局最优)<br />[12.5 监督学习与非监督学习的区别](https://blog.csdn.net/m0_67647321/article/details/138187377#12.5 监督学习与非监督学习的区别)<br />[12.6 监督学习有哪些步骤](https://blog.csdn.net/m0_67647321/article/details/138187377#12.6 监督学习有哪些步骤)<br />[12.7 为什么神经网络越来越深,变深的意义在哪](https://blog.csdn.net/m0_67647321/article/details/138187377#12.7 为什么神经网络越来越深%2C变深的意义在哪)<br />[12.8 为什么深度神经网络较浅层网络难以训练](https://blog.csdn.net/m0_67647321/article/details/138187377#12.8 为什么深度神经网络较浅层网络难以训练)<br />[12.9 超参数搜索过程是怎样的](https://blog.csdn.net/m0_67647321/article/details/138187377#12.9 超参数搜索过程是怎样的)<br />[12.10 模型Fine tuning](https://blog.csdn.net/m0_67647321/article/details/138187377#12.10 模型Fine tuning)<br />[12.11 深度学习为什么不用二阶优化](https://blog.csdn.net/m0_67647321/article/details/138187377#12.11 深度学习为什么不用二阶优化)<br />[12.12 什么是TOP5错误率](https://blog.csdn.net/m0_67647321/article/details/138187377#12.12 什么是TOP5错误率)<br />[12.13 开发平台的选择](https://blog.csdn.net/m0_67647321/article/details/138187377#12.13 开发平台的选择) |
| 传统图像算法                                                 | [13.1 HSV色彩空间](https://blog.csdn.net/m0_67647321/article/details/138681102?spm=1001.2014.3001.5501#13.1 HSV色彩空间)<br />[13.2 swish激活函数](https://blog.csdn.net/m0_67647321/article/details/138681102?spm=1001.2014.3001.5501#13.2 swish激活函数)<br />[13.3 OpenCV——几何变换](https://blog.csdn.net/m0_67647321/article/details/138681102?spm=1001.2014.3001.5501#13.3 OpenCV——几何变换)<br />[13.4 图像平滑处理](https://blog.csdn.net/m0_67647321/article/details/138681102?spm=1001.2014.3001.5501#13.4 图像平滑处理)<br />[13.5 图像梯度](https://blog.csdn.net/m0_67647321/article/details/138681102?spm=1001.2014.3001.5501#13.5 图像梯度) |
| 池化层                                                       | [14.1 什么是池化](https://blog.csdn.net/m0_67647321/article/details/138309213#14.1 什么是池化)<br />[14.2 池化层的作用](https://blog.csdn.net/m0_67647321/article/details/138309213#14.2 池化层的作用)<br />[14.3 平均池化](https://blog.csdn.net/m0_67647321/article/details/138309213#14.3 平均池化)<br />[14.4 最大池化](https://blog.csdn.net/m0_67647321/article/details/138309213#14.4 最大池化)<br />[14.5 空间金字塔池化](https://blog.csdn.net/m0_67647321/article/details/138309213#14.5 空间金字塔池化)<br />[14.6 ROI Pooling](https://blog.csdn.net/m0_67647321/article/details/138309213#14.6 ROI Pooling)<br />[14.7 最大池化与平均池化是如何进行反向传播的](https://blog.csdn.net/m0_67647321/article/details/138309213#14.7 最大池化与平均池化是如何进行反向传播的)<br />[14.8 卷积层与池化层的区别](https://blog.csdn.net/m0_67647321/article/details/138309213#14.8 卷积层与池化层的区别) |
| 支持向量机——SVM                                              | [15.1 SVM](https://blog.csdn.net/m0_67647321/article/details/138397057#151_SVM_10)<br />[15.2 SVM原理](https://blog.csdn.net/m0_67647321/article/details/138397057#152_SVM_31)<br />[15.3 SVM解决问题的类型](https://blog.csdn.net/m0_67647321/article/details/138397057#153_SVM_100)<br />[15.4 核函数的作用以及特点](https://blog.csdn.net/m0_67647321/article/details/138397057#154__123)<br />[15.5 核函数的表达式](https://blog.csdn.net/m0_67647321/article/details/138397057#155__149)<br />[15.6 SVM为什么引入对偶问题](https://blog.csdn.net/m0_67647321/article/details/138397057#156_SVM_162)<br />[15.7 SVM使用SGD及步骤](https://blog.csdn.net/m0_67647321/article/details/138397057#157_SVMSGD_178)<br />[15.8 为什么SVM对缺失数据敏感](https://blog.csdn.net/m0_67647321/article/details/138397057#158_SVM_198)<br />[15.9 SVM怎么防止过拟合](https://blog.csdn.net/m0_67647321/article/details/138397057#159_SVM_210) |
| 模型评价指标                                                 | [16.1 回归模型评估常用的方法](https://blog.csdn.net/m0_67647321/article/details/138610574#16.1 回归模型评估常用的方法)<br />[16.2 混淆矩阵](https://blog.csdn.net/m0_67647321/article/details/138610574#16.2 混淆矩阵)<br />[16.3 查准率，查全率，F1-score，准确率](https://blog.csdn.net/m0_67647321/article/details/138610574#16.3 查准率，查全率，F1-score，准确率)<br />[16.4 PR曲线图](https://blog.csdn.net/m0_67647321/article/details/138610574#16.4 PR曲线图)<br />[16.5 AP与mAP](https://blog.csdn.net/m0_67647321/article/details/138610574#16.5 AP与mAP) |
| 正则优化函数                                                 | [17.1 什么是过拟合和欠拟合](https://blog.csdn.net/m0_67647321/article/details/138707720#17.1 什么是过拟合和欠拟合)<br />[17.2 解决过拟合和欠拟合的方法有哪些](https://blog.csdn.net/m0_67647321/article/details/138707720#17.2 解决过拟合和欠拟合的方法有哪些)<br />[17.3 什么是正则化？](https://blog.csdn.net/m0_67647321/article/details/138707720#17.3 什么是正则化？)<br />[17.4 L1与L2为什么对于特征选择有着不同方式](https://blog.csdn.net/m0_67647321/article/details/138707720#17.4 L1与L2为什么对于特征选择有着不同方式)<br />[17.5 正则化有什么作用](https://blog.csdn.net/m0_67647321/article/details/138707720#17.5 正则化有什么作用)<br />[17.6 介绍一下BN](https://blog.csdn.net/m0_67647321/article/details/138707720#17.6 介绍一下BN)<br />[17.7 BN训练与测试有什么不同](https://blog.csdn.net/m0_67647321/article/details/138707720#17.7 BN训练与测试有什么不同)<br />[17.8 BN/LN/IN/GN区别](https://blog.csdn.net/m0_67647321/article/details/138707720#17.8 BN%2FLN%2FIN%2FGN区别) |
| 特征工程相关理论                                             | [18.1 特征工程有哪些](https://blog.csdn.net/m0_67647321/article/details/138794101#18.1 特征工程有哪些)<br />[18.2 遇到缺值的情况如何处理](https://blog.csdn.net/m0_67647321/article/details/138794101#18.2 遇到缺值的情况如何处理)<br />[18.3 机器学习中解决样本不均衡问题的方法](https://blog.csdn.net/m0_67647321/article/details/138794101#18.3 机器学习中解决样本不均衡问题的方法)<br />[18.4 深度学习中解决样本不均衡问题的方法](https://blog.csdn.net/m0_67647321/article/details/138794101#18.4 深度学习中解决样本不均衡问题的方法)<br />[18.5 常见的特征筛选方法](https://blog.csdn.net/m0_67647321/article/details/138794101#18.5 常见的特征筛选方法)<br />[18.6 特征选择的目的](https://blog.csdn.net/m0_67647321/article/details/138794101#18.6 特征选择的目的)<br />[18.7 训练时出现Nan的原因](https://blog.csdn.net/m0_67647321/article/details/138794101#18.7 训练时出现Nan的原因)<br />[18.8 怎么找出相似性高的特征并去掉](https://blog.csdn.net/m0_67647321/article/details/138794101#18.8 怎么找出相似性高的特征并去掉) |
| 降维                                                         | [19.1 怎样避免维数灾难？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.1 怎样避免维数灾难？)<br />[19.2 降维的必要性？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.2 降维的必要性？)<br />[19.3 降维有什么意义？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.3 降维有什么意义？)<br />[19.4 PCA主成分分析？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.4 PCA主成分分析？)<br />[19.5 PCA核心思想？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.5 PCA核心思想？)<br />[19.6 如何得到包含最大差异性的主成分方向？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.6 如何得到包含最大差异性的主成分方向？)<br />[19.7 特征值分解矩阵](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.7 特征值分解矩阵)<br />[19.8 SVD分解矩阵？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.8 SVD分解矩阵？)<br />[19.9 PCA算法流程总结？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.9 PCA算法流程总结？)<br />[19.10 PCA核心思想](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.10 PCA核心思想)<br />[19.11 PCA降维之后的维度怎么确定？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.11 PCA降维之后的维度怎么确定？)<br />[19.12 PCA有什么优缺点？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.12 PCA有什么优缺点？)<br />[19.13 线性判别分析LDA？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.13 线性判别分析LDA？)<br />[19.14 线性判别分析LDA核心思想？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.14 线性判别分析LDA核心思想？)<br />[19.15 LDA的优缺点？](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.15 LDA的优缺点？)<br />[19.16 PCA和LDA的对比](https://blog.csdn.net/m0_67647321/article/details/138904126?spm=1001.2014.3001.5502#19.16 PCA和LDA的对比) |
| 模型优化                                                     | [21.1 你知道的模型压缩方法有哪些？](https://blog.csdn.net/m0_67647321/article/details/139065255#21.1 你知道的模型压缩方法有哪些？)<br />[21.2 模型压缩的作用与意义](https://blog.csdn.net/m0_67647321/article/details/139065255#21.2 模型压缩的作用与意义)<br />[21.3 谈谈低秩近似](https://blog.csdn.net/m0_67647321/article/details/139065255#21.3 谈谈低秩近似)<br />[21.4 剪枝与稀疏约束](https://blog.csdn.net/m0_67647321/article/details/139065255#21.4 剪枝与稀疏约束)<br />[21.5 参数量化的优缺点](https://blog.csdn.net/m0_67647321/article/details/139065255#21.5 参数量化的优缺点)<br />[21.6 你了解知识蒸馏（Knowledge Distillation）吗](https://blog.csdn.net/m0_67647321/article/details/139065255#21.6 你了解知识蒸馏（Knowledge Distillation）吗)<br />[21.7 降低网络复杂度但不影响精度的方法有哪些](https://blog.csdn.net/m0_67647321/article/details/139065255#21.7 降低网络复杂度但不影响精度的方法有哪些)<br />[21.8 聊聊TensorRT加速原理](https://blog.csdn.net/m0_67647321/article/details/139065255#21.8 聊聊TensorRT加速原理) |
| 集成学习                                                     | [22.1 集成学习方法有哪些？](https://blog.csdn.net/m0_67647321/article/details/139090145#22.1 集成学习方法有哪些？)<br />[22.2 介绍一下Boosting算法](https://blog.csdn.net/m0_67647321/article/details/139090145#22.2 介绍一下Boosting算法)<br />[22.3 介绍一下Bagging算法](https://blog.csdn.net/m0_67647321/article/details/139090145#22.3 介绍一下Bagging算法)<br />[22.4 分别介绍一下AdaBoost、GBDT、XGboost、LightGBM、RF算法](https://blog.csdn.net/m0_67647321/article/details/139090145#22.4 分别介绍一下AdaBoost、GBDT、XGboost、LightGBM、RF算法)<br />[22.5 Adaboost的优缺点](https://blog.csdn.net/m0_67647321/article/details/139090145#22.5 Adaboost的优缺点)<br />[22.6 LightGBM与XGBoost的区别](https://blog.csdn.net/m0_67647321/article/details/139090145#22.6 LightGBM与XGBoost的区别)<br />[22.7 XGBoost、GBDT的区别](https://blog.csdn.net/m0_67647321/article/details/139090145#22.7 XGBoost、GBDT的区别)<br />[22.8 随机森林与GBDT区别](https://blog.csdn.net/m0_67647321/article/details/139090145#22.8 随机森林与GBDT区别)<br />[22.9 GBDT算法步骤](https://blog.csdn.net/m0_67647321/article/details/139090145#22.9 GBDT算法步骤)<br />[22.10 随机森林（Random Forest）中的“随机”体现在哪些方面](https://blog.csdn.net/m0_67647321/article/details/139090145#22.10 随机森林（Random Forest）中的“随机”体现在哪些方面) |
| 贝叶斯网络                                                   | [23.1 先验概率与后验概率是怎么回事](https://blog.csdn.net/m0_67647321/article/details/139112830#23.1 先验概率与后验概率是怎么回事)<br />[23.2 极大似然估计的原理](https://blog.csdn.net/m0_67647321/article/details/139112830#23.2 极大似然估计的原理)<br />[23.3 极大似然估计](https://blog.csdn.net/m0_67647321/article/details/139112830#23.3 极大似然估计)<br />[23.4 朴素贝叶斯](https://blog.csdn.net/m0_67647321/article/details/139112830#23.4 朴素贝叶斯)<br />[23.5 怎么理解朴素贝叶斯的“朴素”](https://blog.csdn.net/m0_67647321/article/details/139112830#23.4 怎么理解朴素贝叶斯的“朴素”)<br />[23.6 朴素贝叶斯中有什么具体应用](https://blog.csdn.net/m0_67647321/article/details/139112830#23.5 朴素贝叶斯中有什么具体应用)<br />[23.7 举例理解朴素贝叶斯分类器（最浅显易懂的经典案例）](https://blog.csdn.net/m0_67647321/article/details/139112830#23.6 举例理解朴素贝叶斯分类器（最浅显易懂的经典案例）)<br />[23.8 什么是拉普拉斯平滑法](https://blog.csdn.net/m0_67647321/article/details/139112830#23.7 什么是拉普拉斯平滑法) |


